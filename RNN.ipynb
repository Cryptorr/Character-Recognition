{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sven Den Hartog, s1003026 \n",
    "\n",
    "Denise Klep, s4210646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.datasets import TupleDataset\n",
    "from chainer.cuda import to_cpu\n",
    "from chainer import serializers\n",
    "import matplotlib.pyplot as plt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create toy data - compute sum of the previous and current input\n",
    "def create_data(n=3000):\n",
    "\n",
    "    X = np.random.rand(n,1).astype('float32')\n",
    "    T = np.sum(np.hstack((X[0:-1],X[1:])),axis=1)\n",
    "    T = np.hstack([0, T[0:]]).astype('float32')\n",
    "    T = T.reshape([n,1])\n",
    "\n",
    "    return TupleDataset(X, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(Chain):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.lstm = L.LSTM(None, 20)  # the first LSTM layer\n",
    "            self.out = L.Linear(None, 1)  # the feed-forward output layer\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.lstm(x)\n",
    "        y = self.out(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73.98312378]]\n",
      "[[ 50.34756088]]\n",
      "[[ 43.77914047]]\n",
      "[[ 36.10275269]]\n",
      "[[ 19.8826561]]\n",
      "[[ 12.75362301]]\n",
      "[[ 8.7898016]]\n",
      "[[ 6.8073163]]\n",
      "[[ 5.65396595]]\n",
      "[[ 4.95727825]]\n",
      "[[ 4.50601959]]\n",
      "[[ 4.19823837]]\n",
      "[[ 3.97256136]]\n",
      "[[ 3.79532981]]\n",
      "[[ 3.64487171]]\n",
      "[[ 3.50967669]]\n",
      "[[ 3.38353634]]\n",
      "[[ 3.26313162]]\n",
      "[[ 3.1484704]]\n",
      "[[ 3.03853011]]\n"
     ]
    }
   ],
   "source": [
    "mean_acc = [] \n",
    "mean_test_loss = [] \n",
    "mean_train_loss = []\n",
    "train_losses = []\n",
    "seqlen = 3000\n",
    "epoch = 0\n",
    "max_epoch = 20\n",
    "trainset = np.array(create_data())\n",
    "testset = np.array(create_data())\n",
    "while epoch < max_epoch:\n",
    "    model.reset_state()\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    for i,x in enumerate(trainset):\n",
    "        prediction = model(np.expand_dims(x[0]-1,0))\n",
    "        loss += ((prediction + 1) - (trainset[i,1,0]))**2\n",
    "        count += 1\n",
    "        if count % 30 == 0 or count == seqlen:\n",
    "            model.cleargrads() #renew gradient calculations\n",
    "            loss.backward() #runs error backpropagation\n",
    "            loss.unchain_backward()\n",
    "            optimizer.update() #update variables\n",
    "    #print(loss.data)\n",
    "    train_losses.append(loss.data)\n",
    "    \n",
    "    loss_test = 0\n",
    "    for i,x in enumerate(testset):\n",
    "        prediction = model(np.expand_dims(x[0]-1,0))\n",
    "        loss_test += ((prediction + 1) - (testset[i,1,0]))**2\n",
    "    print(loss_test.data)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize results for training and test loss values over the epochs\n",
    "plt.xlabel('nr of epochs')\n",
    "plt.ylabel('loss')\n",
    "epochs = range(0,np.size(mean_train_loss))\n",
    "plt.plot(epochs,mean_train_loss,\n",
    "         label='training loss')\n",
    "plt.plot(epochs,mean_test_loss,\n",
    "         label='test loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('MLP MNIST classification results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
