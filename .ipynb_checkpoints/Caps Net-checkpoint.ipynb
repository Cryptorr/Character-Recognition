{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import *\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.dataset import concat_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caps(Chain):\n",
    "    def __init__(self):\n",
    "        super(Caps, self).__init__()\n",
    "        self.n_iterations = 3\n",
    "        self.n_grids = 6  # grid width of primary capsules layer\n",
    "        self.n_raw_grids = self.n_grids\n",
    "        self.init = initializers.Uniform(scale=0.05)\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(3, 256, ksize=9, stride=1, initialW=self.init)\n",
    "            self.conv2 = L.Convolution2D(256, 32 * 8, ksize=9, stride=2, initialW=self.init)\n",
    "            self.Ws = ChainList(\n",
    "                *[L.Convolution2D(8, 16 * 100, ksize=1, stride=1, initialW=self.init)\n",
    "                  for i in range(32)])\n",
    "            \n",
    "    def __call__(self, x, t):\n",
    "        vs_norm, vs = self.output(x)\n",
    "        return self.calculate_loss(vs_norm, t, vs, x)\n",
    "    \n",
    "    def output(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        n_iters = self.n_iterations\n",
    "        gg = self.n_grids * self.n_grids\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x))\n",
    "        pr_caps = F.split_axis(self.conv2(h1), 32, axis=1)\n",
    "\n",
    "        Preds = []\n",
    "        for i in range(32):\n",
    "            pred = self.Ws[i](pr_caps[i])\n",
    "            Pred = pred.reshape((batchsize, 16, 100, gg))\n",
    "            Preds.append(Pred)\n",
    "        Preds = F.stack(Preds, axis=3)\n",
    "\n",
    "        bs = self.xp.zeros((batchsize, 100, 32, gg), dtype='f')\n",
    "        for i_iter in range(n_iters):\n",
    "            cs = F.softmax(bs, axis=1)\n",
    "            Cs = F.broadcast_to(cs[:, None], Preds.shape)\n",
    "            ss = F.sum(Cs * Preds, axis=(3, 4))\n",
    "            vs = self.squash(ss)\n",
    "\n",
    "            if i_iter != n_iters - 1:\n",
    "                Vs = F.broadcast_to(vs[:, :, :, None, None], Preds.shape)\n",
    "                bs = bs + F.sum(Vs * Preds, axis=1)\n",
    "\n",
    "        vs_norm = F.sqrt(F.sum(vs ** 2, axis=1))\n",
    "        return vs_norm, vs\n",
    "    \n",
    "    def calculate_loss(self, vs_norm, t, vs, x):\n",
    "        xp = self.xp\n",
    "        batchsize = t.shape[0]\n",
    "        I = xp.arange(batchsize)\n",
    "        T = xp.zeros(vs_norm.shape, dtype='f')\n",
    "        T[I, t] = 1.\n",
    "        m = xp.full(vs_norm.shape, 0.1, dtype='f')\n",
    "        m[I, t] = 0.9\n",
    "\n",
    "        loss = T * F.relu(m - vs_norm) ** 2 + \\\n",
    "            0.5 * (1. - T) * F.relu(vs_norm - m) ** 2\n",
    "        return F.sum(loss) / batchsize\n",
    "    \n",
    "    def squash(self, ss):\n",
    "        ss_norm2 = F.sum(ss ** 2, axis=1, keepdims=True)\n",
    "        norm_div_1pnorm2 = F.sqrt(ss_norm2) / (1. + ss_norm2)\n",
    "        norm_div_1pnorm2 = F.broadcast_to(norm_div_1pnorm2, ss.shape)\n",
    "        vs = norm_div_1pnorm2 * ss\n",
    "        return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "data = datasets.get_cifar100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Caps()\n",
    "optimizer = optimizers.Adam(alpha=1e-3)\n",
    "optimizer.setup(model)\n",
    "\n",
    "max_epoch = 10\n",
    "batchsize = 32\n",
    "\n",
    "train, test = data\n",
    "train_iter = iterators.SerialIterator(train, batchsize)\n",
    "test_iter = iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)\n",
    "mean_test_loss = []\n",
    "mean_train_loss = []\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while train_iter.epoch < max_epoch:\n",
    "\n",
    "    train_batch = train_iter.next()\n",
    "    image_train, target_train = concat_examples(train_batch)\n",
    "    image_train = image_train[:,:,2:-2,2:-2]\n",
    "    \n",
    "    loss = model(image_train, target_train)\n",
    "    train_losses.append(loss.data)\n",
    "    print(loss.data)\n",
    "    model.cleargrads() #renew gradient calculations\n",
    "    loss.backward() #runs error backpropagation\n",
    "\n",
    "    optimizer.update() #update variables\n",
    "    if train_iter.is_new_epoch:\n",
    "        print('epoch {0:2d}'.format(train_iter.epoch))\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "            test_batch = test_iter.next()\n",
    "            image_test, target_test = concat_examples(test_batch)\n",
    "            image_test = image_test[:,:,2:-2,2:-2]\n",
    "            loss_test = model(image_test, target_test)\n",
    "            test_losses.append(loss_test.data)\n",
    "            \n",
    "            if test_iter.is_new_epoch:\n",
    "                test_iter.epoch = 0\n",
    "                test_iter.current_position = 0\n",
    "                test_iter.is_new_epoch = False\n",
    "                test_iter._pushed_position = None\n",
    "                break\n",
    "\n",
    "        mean_test_loss.append(np.mean(test_losses))\n",
    "        mean_train_loss.append(np.mean(train_losses))\n",
    "        #track mean losses for visualization later\n",
    "        train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('nr of epochs')\n",
    "plt.ylabel('loss')\n",
    "epochs = range(0,np.size(mean_train_loss))\n",
    "plt.plot(epochs,mean_train_loss)\n",
    "plt.title('Caps Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
