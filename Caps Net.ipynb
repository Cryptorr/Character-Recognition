{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import *\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.dataset import concat_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caps(Chain):\n",
    "    def __init__(self, n_output, channels, n_primary=32, d_primary=8, n_grids=6, d_output=16, n_iterations=3):\n",
    "        super(Caps, self).__init__()\n",
    "        self.n_iterations = n_iterations\n",
    "        self.n_grids = n_grids\n",
    "        self.n_raw_grids = self.n_grids\n",
    "        self.n_primary = n_primary\n",
    "        self.d_primary = d_primary\n",
    "        self.n_output = n_output\n",
    "        self.d_output = d_output\n",
    "        self.init = initializers.Uniform(scale=0.05)\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(channels, 64, ksize=9, stride=1, initialW=self.init)\n",
    "            self.conv2 = L.Convolution2D(64, n_primary * d_primary, ksize=9, stride=2, initialW=self.init)\n",
    "            self.Ws = ChainList(\n",
    "                *[L.Convolution2D(d_primary, n_output * d_output, ksize=1, stride=1, initialW=self.init)\n",
    "                  for i in range(n_primary)])\n",
    "            \n",
    "    def __call__(self, x, t):\n",
    "        vs_norm, vs = self.output(x)\n",
    "        return self.calculate_loss(vs_norm, t, vs, x)\n",
    "    \n",
    "    def output(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        n_iters = self.n_iterations\n",
    "        d_prim = self.d_primary\n",
    "        n_prim = self.n_primary\n",
    "        d_out = self.d_output\n",
    "        n_out = self.n_output\n",
    "        gg = self.n_grids * self.n_grids\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x))\n",
    "        pr_caps = F.split_axis(self.conv2(h1), n_prim, axis=1)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(n_prim):\n",
    "            pred = self.Ws[i](pr_caps[i])\n",
    "            pred = pred.reshape((batchsize, d_out, n_out, gg))\n",
    "            preds.append(Pred)\n",
    "        preds = F.stack(preds, axis=3)\n",
    "\n",
    "        bs = self.xp.zeros((batchsize, n_out, n_prim, gg), dtype='f')\n",
    "        for i_iter in range(n_iters):\n",
    "            cs = F.softmax(bs, axis=1)\n",
    "            Cs = F.broadcast_to(cs[:, None], preds.shape)\n",
    "            ss = F.sum(Cs * preds, axis=(3, 4))\n",
    "            vs = self.squash(ss)\n",
    "\n",
    "            if i_iter != n_iters - 1:\n",
    "                Vs = F.broadcast_to(vs[:, :, :, None, None], preds.shape)\n",
    "                bs = bs + F.sum(Vs * preds, axis=1)\n",
    "\n",
    "        vs_norm = F.sqrt(F.sum(vs ** 2, axis=1))\n",
    "        return vs_norm, vs\n",
    "    \n",
    "    def calculate_loss(self, vs_norm, t, vs, x):\n",
    "        xp = self.xp\n",
    "        batchsize = t.shape[0]\n",
    "        I = xp.arange(batchsize)\n",
    "        T = xp.zeros(vs_norm.shape, dtype='f')\n",
    "        T[I, t] = 1.\n",
    "        m = xp.full(vs_norm.shape, 0.1, dtype='f')\n",
    "        m[I, t] = 0.9\n",
    "\n",
    "        loss = T * F.relu(m - vs_norm) ** 2 + \\\n",
    "            0.5 * (1. - T) * F.relu(vs_norm - m) ** 2\n",
    "        return F.sum(loss) / batchsize\n",
    "    \n",
    "    def squash(self, ss):\n",
    "        ss_norm2 = F.sum(ss ** 2, axis=1, keepdims=True)\n",
    "        norm_div_1pnorm2 = F.sqrt(ss_norm2) / (1. + ss_norm2)\n",
    "        norm_div_1pnorm2 = F.broadcast_to(norm_div_1pnorm2, ss.shape)\n",
    "        vs = norm_div_1pnorm2 * ss\n",
    "        return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = datasets.get_cifar100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Caps(100, 3, n_primary=16, d_primary=6, d_output=8)\n",
    "optimizer = optimizers.Adam(alpha=1e-3)\n",
    "optimizer.setup(model)\n",
    "\n",
    "max_epoch = 10\n",
    "batchsize = 32\n",
    "\n",
    "train, test = data\n",
    "train_iter = iterators.SerialIterator(train, batchsize)\n",
    "test_iter = iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)\n",
    "mean_test_loss = []\n",
    "mean_train_loss = []\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7952632904052734\n",
      "0.919095516204834\n",
      "0.780173659324646\n",
      "0.7952120304107666\n",
      "0.7902535796165466\n",
      "0.7897486090660095\n",
      "0.7807108163833618\n",
      "0.7729950547218323\n",
      "0.7544610500335693\n",
      "0.7346677184104919\n",
      "0.7425596714019775\n",
      "0.7218933701515198\n",
      "0.7102875709533691\n",
      "0.6971392035484314\n",
      "0.6957331299781799\n",
      "0.6944629549980164\n",
      "0.6982715725898743\n",
      "0.7002586126327515\n",
      "0.701023519039154\n",
      "0.6970616579055786\n",
      "0.7029879093170166\n",
      "0.7004846334457397\n",
      "0.6884542107582092\n",
      "0.6963273286819458\n",
      "0.6957551836967468\n",
      "0.6852080225944519\n",
      "0.695458710193634\n",
      "0.7028652429580688\n",
      "0.6795175671577454\n",
      "0.6699709892272949\n",
      "0.6788751482963562\n",
      "0.6681269407272339\n",
      "0.6784891486167908\n",
      "0.6891242861747742\n",
      "0.6739841103553772\n",
      "0.6841603517532349\n",
      "0.6913002729415894\n",
      "0.6860209107398987\n",
      "0.6781191825866699\n",
      "0.6775566339492798\n",
      "0.6577186584472656\n",
      "0.6635931730270386\n",
      "0.6668238639831543\n",
      "0.6631033420562744\n",
      "0.6629984974861145\n",
      "0.66387939453125\n",
      "0.6676069498062134\n",
      "0.6649587154388428\n",
      "0.6572043895721436\n",
      "0.6580466032028198\n",
      "0.6689885854721069\n",
      "0.6537253856658936\n",
      "0.6493048071861267\n",
      "0.6420444846153259\n",
      "0.6601457595825195\n",
      "0.643589437007904\n",
      "0.6488887071609497\n",
      "0.6513766050338745\n",
      "0.6572574377059937\n",
      "0.650536060333252\n",
      "0.663203239440918\n",
      "0.6387757062911987\n",
      "0.6556578278541565\n",
      "0.6521056294441223\n",
      "0.646224856376648\n",
      "0.6530289649963379\n",
      "0.6476227045059204\n",
      "0.6460863351821899\n",
      "0.6622278094291687\n",
      "0.6575002670288086\n",
      "0.6488199234008789\n",
      "0.6480975151062012\n",
      "0.6466490030288696\n",
      "0.648248553276062\n",
      "0.6345826387405396\n",
      "0.6460609436035156\n",
      "0.6330074071884155\n",
      "0.6484516859054565\n",
      "0.6401322484016418\n",
      "0.650747537612915\n",
      "0.6329337358474731\n",
      "0.6441472768783569\n",
      "0.6372010707855225\n",
      "0.6323032975196838\n",
      "0.6372605562210083\n",
      "0.6320276260375977\n",
      "0.6501899361610413\n",
      "0.6429615616798401\n",
      "0.6462811827659607\n",
      "0.631874144077301\n",
      "0.630881667137146\n",
      "0.6262909770011902\n",
      "0.6387827396392822\n",
      "0.6419945955276489\n",
      "0.6273849010467529\n",
      "0.6330529451370239\n",
      "0.6299320459365845\n",
      "0.627299964427948\n",
      "0.6235100626945496\n",
      "0.6228852272033691\n",
      "0.6362053751945496\n",
      "0.6316747069358826\n",
      "0.6318306922912598\n",
      "0.626568078994751\n",
      "0.6314548254013062\n",
      "0.6411353349685669\n",
      "0.6275074481964111\n",
      "0.6469920873641968\n",
      "0.6311777830123901\n",
      "0.6319165229797363\n",
      "0.6306430697441101\n",
      "0.6252947449684143\n",
      "0.6338680386543274\n",
      "0.6372717618942261\n",
      "0.6252150535583496\n",
      "0.6387535929679871\n",
      "0.6272187232971191\n",
      "0.6338227987289429\n",
      "0.6215242147445679\n",
      "0.629754900932312\n",
      "0.6277995109558105\n",
      "0.6401387453079224\n",
      "0.6343245506286621\n",
      "0.6386849284172058\n",
      "0.6237164735794067\n",
      "0.6248161792755127\n",
      "0.6335991621017456\n",
      "0.6377209424972534\n",
      "0.637236475944519\n",
      "0.6288745999336243\n",
      "0.6231931447982788\n",
      "0.6390470266342163\n",
      "0.6359165906906128\n",
      "0.6291052103042603\n",
      "0.6301438808441162\n",
      "0.6377314329147339\n",
      "0.6294068694114685\n",
      "0.6373293399810791\n",
      "0.6267740726470947\n",
      "0.6193163394927979\n",
      "0.6310957670211792\n",
      "0.6357059478759766\n",
      "0.6127979755401611\n",
      "0.6233810186386108\n",
      "0.626089334487915\n",
      "0.6360998153686523\n",
      "0.6285096406936646\n",
      "0.6177681684494019\n",
      "0.6348069906234741\n",
      "0.6249011158943176\n",
      "0.6270573139190674\n",
      "0.6339494585990906\n",
      "0.6340051889419556\n",
      "0.6215981841087341\n",
      "0.628746747970581\n",
      "0.6352683305740356\n",
      "0.6322280168533325\n",
      "0.6199442148208618\n",
      "0.6269897818565369\n",
      "0.6257901191711426\n",
      "0.6387530565261841\n",
      "0.6409334540367126\n",
      "0.6199787855148315\n",
      "0.6384707689285278\n",
      "0.6399405598640442\n",
      "0.624204695224762\n",
      "0.6279444098472595\n",
      "0.6335114240646362\n",
      "0.6246770620346069\n",
      "0.6314684152603149\n",
      "0.6341290473937988\n",
      "0.626534640789032\n",
      "0.6187387704849243\n",
      "0.6230374574661255\n",
      "0.6262729167938232\n",
      "0.6400168538093567\n",
      "0.6311031579971313\n",
      "0.6302988529205322\n",
      "0.6278467178344727\n",
      "0.6214205622673035\n",
      "0.6202569007873535\n",
      "0.6130664348602295\n",
      "0.6191543340682983\n",
      "0.6180988550186157\n",
      "0.6135174036026001\n",
      "0.6357264518737793\n",
      "0.624879002571106\n",
      "0.6319261789321899\n",
      "0.6438742876052856\n",
      "0.6388527750968933\n",
      "0.625638484954834\n",
      "0.638279139995575\n",
      "0.6387000679969788\n",
      "0.6372127532958984\n",
      "0.6267603635787964\n",
      "0.6275765895843506\n",
      "0.6310620307922363\n",
      "0.6328647136688232\n",
      "0.6325961351394653\n",
      "0.622965931892395\n",
      "0.634549081325531\n",
      "0.6160247325897217\n",
      "0.6309596300125122\n",
      "0.6302325129508972\n",
      "0.6311938166618347\n",
      "0.6232860088348389\n",
      "0.6315248608589172\n",
      "0.6182250380516052\n",
      "0.6081526279449463\n",
      "0.6226016283035278\n",
      "0.6339954137802124\n",
      "0.6322015523910522\n",
      "0.6217078566551208\n",
      "0.6265192031860352\n",
      "0.6351344585418701\n",
      "0.6266075372695923\n",
      "0.6258864402770996\n",
      "0.6259342432022095\n",
      "0.6466891765594482\n",
      "0.631562352180481\n",
      "0.6276644468307495\n",
      "0.6271530985832214\n",
      "0.6268649697303772\n",
      "0.6326434016227722\n",
      "0.6199613809585571\n",
      "0.6354632377624512\n",
      "0.6337970495223999\n",
      "0.6136611700057983\n",
      "0.6228668689727783\n",
      "0.6263785362243652\n",
      "0.6361760497093201\n",
      "0.6135929822921753\n",
      "0.6332488059997559\n",
      "0.6326446533203125\n",
      "0.6296055912971497\n",
      "0.6285091042518616\n",
      "0.6329541802406311\n",
      "0.6313555836677551\n",
      "0.6235994100570679\n",
      "0.6255828142166138\n",
      "0.6225817203521729\n",
      "0.6350650191307068\n",
      "0.6318763494491577\n",
      "0.62834632396698\n",
      "0.6298911571502686\n",
      "0.6294450759887695\n",
      "0.6243578195571899\n",
      "0.6316498517990112\n",
      "0.633049488067627\n",
      "0.6336503028869629\n",
      "0.6152846813201904\n",
      "0.6351180672645569\n",
      "0.6364250183105469\n",
      "0.6345510482788086\n",
      "0.6156047582626343\n",
      "0.6413611173629761\n",
      "0.6154441833496094\n",
      "0.6356649398803711\n",
      "0.6206505298614502\n",
      "0.6362179517745972\n"
     ]
    }
   ],
   "source": [
    "while train_iter.epoch < max_epoch:\n",
    "\n",
    "    train_batch = train_iter.next()\n",
    "    image_train, target_train = concat_examples(train_batch)\n",
    "    image_train = image_train[:,:,2:-2,2:-2]\n",
    "    \n",
    "    loss = model(image_train, target_train)\n",
    "    train_losses.append(loss.data)\n",
    "    print(loss.data)\n",
    "    model.cleargrads() #renew gradient calculations\n",
    "    loss.backward() #runs error backpropagation\n",
    "\n",
    "    optimizer.update() #update variables\n",
    "    if train_iter.is_new_epoch:\n",
    "        print('epoch {0:2d}'.format(train_iter.epoch))\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "            test_batch = test_iter.next()\n",
    "            image_test, target_test = concat_examples(test_batch)\n",
    "            image_test = image_test[:,:,2:-2,2:-2]\n",
    "            loss_test = model(image_test, target_test)\n",
    "            test_losses.append(loss_test.data)\n",
    "            \n",
    "            if test_iter.is_new_epoch:\n",
    "                test_iter.epoch = 0\n",
    "                test_iter.current_position = 0\n",
    "                test_iter.is_new_epoch = False\n",
    "                test_iter._pushed_position = None\n",
    "                break\n",
    "\n",
    "        mean_test_loss.append(np.mean(test_losses))\n",
    "        mean_train_loss.append(np.mean(train_losses))\n",
    "        #track mean losses for visualization later\n",
    "        train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('nr of epochs')\n",
    "plt.ylabel('loss')\n",
    "epochs = range(0,np.size(mean_train_loss))\n",
    "plt.plot(epochs,mean_train_loss)\n",
    "plt.plot(epochs,mean_test_loss)\n",
    "plt.title('Caps Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
